{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac69ea97-35fa-49b2-903e-d45f25666080",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5463d024-7b7b-4e17-882b-253b3a1bb7a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.0165295 , -0.02112558, -0.0169932 , ..., -0.00256228,\n",
       "        -0.00088989, -0.02896755],\n",
       "       [-0.03935254,  0.00195533, -0.00018787, ..., -0.0106814 ,\n",
       "        -0.00777915, -0.01328851],\n",
       "       [-0.01361533,  0.01041415,  0.02203733, ...,  0.00039263,\n",
       "        -0.02203733, -0.02931899],\n",
       "       ...,\n",
       "       [-0.00749965, -0.0001639 ,  0.00786398, ...,  0.00833829,\n",
       "        -0.02132348, -0.02975113],\n",
       "       [ 0.00186836,  0.00824581,  0.00579642, ...,  0.00642269,\n",
       "        -0.0217105 , -0.02240635],\n",
       "       [-0.01477474,  0.00649458, -0.00924997, ...,  0.01485877,\n",
       "        -0.02139886, -0.03568345]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_embeddings = np.load(\"test_case_embeddings.npy\")\n",
    "loaded_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87f81133-76ec-4acf-8d0e-479e6343718b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning DBSCAN for OpenAI Model (text-embedding-ada-002):\n",
      "eps: 0.1, min_samples: 2, Clusters: 51, Noise points: 1647\n",
      "eps: 0.1, min_samples: 3, Clusters: 4, Noise points: 1741\n",
      "eps: 0.1, min_samples: 4, Clusters: 1, Noise points: 1750\n",
      "eps: 0.1, min_samples: 5, Clusters: 0, Noise points: 1754\n",
      "eps: 0.1, min_samples: 6, Clusters: 0, Noise points: 1754\n",
      "eps: 0.1, min_samples: 7, Clusters: 0, Noise points: 1754\n",
      "eps: 0.1, min_samples: 8, Clusters: 0, Noise points: 1754\n",
      "eps: 0.1, min_samples: 9, Clusters: 0, Noise points: 1754\n",
      "eps: 0.2, min_samples: 2, Clusters: 128, Noise points: 1466\n",
      "eps: 0.2, min_samples: 3, Clusters: 17, Noise points: 1688\n",
      "eps: 0.2, min_samples: 4, Clusters: 11, Noise points: 1706\n",
      "eps: 0.2, min_samples: 5, Clusters: 1, Noise points: 1746\n",
      "eps: 0.2, min_samples: 6, Clusters: 0, Noise points: 1754\n",
      "eps: 0.2, min_samples: 7, Clusters: 0, Noise points: 1754\n",
      "eps: 0.2, min_samples: 8, Clusters: 0, Noise points: 1754\n",
      "eps: 0.2, min_samples: 9, Clusters: 0, Noise points: 1754\n",
      "eps: 0.30000000000000004, min_samples: 2, Clusters: 155, Noise points: 1366\n",
      "eps: 0.30000000000000004, min_samples: 3, Clusters: 21, Noise points: 1634\n",
      "eps: 0.30000000000000004, min_samples: 4, Clusters: 10, Noise points: 1670\n",
      "eps: 0.30000000000000004, min_samples: 5, Clusters: 5, Noise points: 1690\n",
      "eps: 0.30000000000000004, min_samples: 6, Clusters: 5, Noise points: 1693\n",
      "eps: 0.30000000000000004, min_samples: 7, Clusters: 3, Noise points: 1705\n",
      "eps: 0.30000000000000004, min_samples: 8, Clusters: 3, Noise points: 1705\n",
      "eps: 0.30000000000000004, min_samples: 9, Clusters: 2, Noise points: 1714\n",
      "eps: 0.4, min_samples: 2, Clusters: 193, Noise points: 916\n",
      "eps: 0.4, min_samples: 3, Clusters: 67, Noise points: 1168\n",
      "eps: 0.4, min_samples: 4, Clusters: 37, Noise points: 1292\n",
      "eps: 0.4, min_samples: 5, Clusters: 28, Noise points: 1353\n",
      "eps: 0.4, min_samples: 6, Clusters: 20, Noise points: 1415\n",
      "eps: 0.4, min_samples: 7, Clusters: 13, Noise points: 1465\n",
      "eps: 0.4, min_samples: 8, Clusters: 12, Noise points: 1478\n",
      "eps: 0.4, min_samples: 9, Clusters: 12, Noise points: 1499\n",
      "eps: 0.5, min_samples: 2, Clusters: 21, Noise points: 126\n",
      "eps: 0.5, min_samples: 3, Clusters: 4, Noise points: 160\n",
      "eps: 0.5, min_samples: 4, Clusters: 2, Noise points: 183\n",
      "eps: 0.5, min_samples: 5, Clusters: 2, Noise points: 199\n",
      "eps: 0.5, min_samples: 6, Clusters: 2, Noise points: 230\n",
      "eps: 0.5, min_samples: 7, Clusters: 2, Noise points: 244\n",
      "eps: 0.5, min_samples: 8, Clusters: 1, Noise points: 269\n",
      "eps: 0.5, min_samples: 9, Clusters: 1, Noise points: 296\n",
      "eps: 0.6, min_samples: 2, Clusters: 1, Noise points: 0\n",
      "eps: 0.6, min_samples: 3, Clusters: 1, Noise points: 0\n",
      "eps: 0.6, min_samples: 4, Clusters: 1, Noise points: 0\n",
      "eps: 0.6, min_samples: 5, Clusters: 1, Noise points: 0\n",
      "eps: 0.6, min_samples: 6, Clusters: 1, Noise points: 1\n",
      "eps: 0.6, min_samples: 7, Clusters: 1, Noise points: 1\n",
      "eps: 0.6, min_samples: 8, Clusters: 1, Noise points: 1\n",
      "eps: 0.6, min_samples: 9, Clusters: 1, Noise points: 1\n",
      "eps: 0.7000000000000001, min_samples: 2, Clusters: 1, Noise points: 0\n",
      "eps: 0.7000000000000001, min_samples: 3, Clusters: 1, Noise points: 0\n",
      "eps: 0.7000000000000001, min_samples: 4, Clusters: 1, Noise points: 0\n",
      "eps: 0.7000000000000001, min_samples: 5, Clusters: 1, Noise points: 0\n",
      "eps: 0.7000000000000001, min_samples: 6, Clusters: 1, Noise points: 0\n",
      "eps: 0.7000000000000001, min_samples: 7, Clusters: 1, Noise points: 0\n",
      "eps: 0.7000000000000001, min_samples: 8, Clusters: 1, Noise points: 0\n",
      "eps: 0.7000000000000001, min_samples: 9, Clusters: 1, Noise points: 0\n",
      "eps: 0.8, min_samples: 2, Clusters: 1, Noise points: 0\n",
      "eps: 0.8, min_samples: 3, Clusters: 1, Noise points: 0\n",
      "eps: 0.8, min_samples: 4, Clusters: 1, Noise points: 0\n",
      "eps: 0.8, min_samples: 5, Clusters: 1, Noise points: 0\n",
      "eps: 0.8, min_samples: 6, Clusters: 1, Noise points: 0\n",
      "eps: 0.8, min_samples: 7, Clusters: 1, Noise points: 0\n",
      "eps: 0.8, min_samples: 8, Clusters: 1, Noise points: 0\n",
      "eps: 0.8, min_samples: 9, Clusters: 1, Noise points: 0\n",
      "eps: 0.9, min_samples: 2, Clusters: 1, Noise points: 0\n",
      "eps: 0.9, min_samples: 3, Clusters: 1, Noise points: 0\n",
      "eps: 0.9, min_samples: 4, Clusters: 1, Noise points: 0\n",
      "eps: 0.9, min_samples: 5, Clusters: 1, Noise points: 0\n",
      "eps: 0.9, min_samples: 6, Clusters: 1, Noise points: 0\n",
      "eps: 0.9, min_samples: 7, Clusters: 1, Noise points: 0\n",
      "eps: 0.9, min_samples: 8, Clusters: 1, Noise points: 0\n",
      "eps: 0.9, min_samples: 9, Clusters: 1, Noise points: 0\n",
      "eps: 1.0, min_samples: 2, Clusters: 1, Noise points: 0\n",
      "eps: 1.0, min_samples: 3, Clusters: 1, Noise points: 0\n",
      "eps: 1.0, min_samples: 4, Clusters: 1, Noise points: 0\n",
      "eps: 1.0, min_samples: 5, Clusters: 1, Noise points: 0\n",
      "eps: 1.0, min_samples: 6, Clusters: 1, Noise points: 0\n",
      "eps: 1.0, min_samples: 7, Clusters: 1, Noise points: 0\n",
      "eps: 1.0, min_samples: 8, Clusters: 1, Noise points: 0\n",
      "eps: 1.0, min_samples: 9, Clusters: 1, Noise points: 0\n",
      "eps: 1.1, min_samples: 2, Clusters: 1, Noise points: 0\n",
      "eps: 1.1, min_samples: 3, Clusters: 1, Noise points: 0\n",
      "eps: 1.1, min_samples: 4, Clusters: 1, Noise points: 0\n",
      "eps: 1.1, min_samples: 5, Clusters: 1, Noise points: 0\n",
      "eps: 1.1, min_samples: 6, Clusters: 1, Noise points: 0\n",
      "eps: 1.1, min_samples: 7, Clusters: 1, Noise points: 0\n",
      "eps: 1.1, min_samples: 8, Clusters: 1, Noise points: 0\n",
      "eps: 1.1, min_samples: 9, Clusters: 1, Noise points: 0\n",
      "eps: 1.2000000000000002, min_samples: 2, Clusters: 1, Noise points: 0\n",
      "eps: 1.2000000000000002, min_samples: 3, Clusters: 1, Noise points: 0\n",
      "eps: 1.2000000000000002, min_samples: 4, Clusters: 1, Noise points: 0\n",
      "eps: 1.2000000000000002, min_samples: 5, Clusters: 1, Noise points: 0\n",
      "eps: 1.2000000000000002, min_samples: 6, Clusters: 1, Noise points: 0\n",
      "eps: 1.2000000000000002, min_samples: 7, Clusters: 1, Noise points: 0\n",
      "eps: 1.2000000000000002, min_samples: 8, Clusters: 1, Noise points: 0\n",
      "eps: 1.2000000000000002, min_samples: 9, Clusters: 1, Noise points: 0\n",
      "eps: 1.3000000000000003, min_samples: 2, Clusters: 1, Noise points: 0\n",
      "eps: 1.3000000000000003, min_samples: 3, Clusters: 1, Noise points: 0\n",
      "eps: 1.3000000000000003, min_samples: 4, Clusters: 1, Noise points: 0\n",
      "eps: 1.3000000000000003, min_samples: 5, Clusters: 1, Noise points: 0\n",
      "eps: 1.3000000000000003, min_samples: 6, Clusters: 1, Noise points: 0\n",
      "eps: 1.3000000000000003, min_samples: 7, Clusters: 1, Noise points: 0\n",
      "eps: 1.3000000000000003, min_samples: 8, Clusters: 1, Noise points: 0\n",
      "eps: 1.3000000000000003, min_samples: 9, Clusters: 1, Noise points: 0\n",
      "eps: 1.4000000000000001, min_samples: 2, Clusters: 1, Noise points: 0\n",
      "eps: 1.4000000000000001, min_samples: 3, Clusters: 1, Noise points: 0\n",
      "eps: 1.4000000000000001, min_samples: 4, Clusters: 1, Noise points: 0\n",
      "eps: 1.4000000000000001, min_samples: 5, Clusters: 1, Noise points: 0\n",
      "eps: 1.4000000000000001, min_samples: 6, Clusters: 1, Noise points: 0\n",
      "eps: 1.4000000000000001, min_samples: 7, Clusters: 1, Noise points: 0\n",
      "eps: 1.4000000000000001, min_samples: 8, Clusters: 1, Noise points: 0\n",
      "eps: 1.4000000000000001, min_samples: 9, Clusters: 1, Noise points: 0\n",
      "eps: 1.5000000000000002, min_samples: 2, Clusters: 1, Noise points: 0\n",
      "eps: 1.5000000000000002, min_samples: 3, Clusters: 1, Noise points: 0\n",
      "eps: 1.5000000000000002, min_samples: 4, Clusters: 1, Noise points: 0\n",
      "eps: 1.5000000000000002, min_samples: 5, Clusters: 1, Noise points: 0\n",
      "eps: 1.5000000000000002, min_samples: 6, Clusters: 1, Noise points: 0\n",
      "eps: 1.5000000000000002, min_samples: 7, Clusters: 1, Noise points: 0\n",
      "eps: 1.5000000000000002, min_samples: 8, Clusters: 1, Noise points: 0\n",
      "eps: 1.5000000000000002, min_samples: 9, Clusters: 1, Noise points: 0\n",
      "eps: 1.6, min_samples: 2, Clusters: 1, Noise points: 0\n",
      "eps: 1.6, min_samples: 3, Clusters: 1, Noise points: 0\n",
      "eps: 1.6, min_samples: 4, Clusters: 1, Noise points: 0\n",
      "eps: 1.6, min_samples: 5, Clusters: 1, Noise points: 0\n",
      "eps: 1.6, min_samples: 6, Clusters: 1, Noise points: 0\n",
      "eps: 1.6, min_samples: 7, Clusters: 1, Noise points: 0\n",
      "eps: 1.6, min_samples: 8, Clusters: 1, Noise points: 0\n",
      "eps: 1.6, min_samples: 9, Clusters: 1, Noise points: 0\n",
      "eps: 1.7000000000000002, min_samples: 2, Clusters: 1, Noise points: 0\n",
      "eps: 1.7000000000000002, min_samples: 3, Clusters: 1, Noise points: 0\n",
      "eps: 1.7000000000000002, min_samples: 4, Clusters: 1, Noise points: 0\n",
      "eps: 1.7000000000000002, min_samples: 5, Clusters: 1, Noise points: 0\n",
      "eps: 1.7000000000000002, min_samples: 6, Clusters: 1, Noise points: 0\n",
      "eps: 1.7000000000000002, min_samples: 7, Clusters: 1, Noise points: 0\n",
      "eps: 1.7000000000000002, min_samples: 8, Clusters: 1, Noise points: 0\n",
      "eps: 1.7000000000000002, min_samples: 9, Clusters: 1, Noise points: 0\n",
      "eps: 1.8000000000000003, min_samples: 2, Clusters: 1, Noise points: 0\n",
      "eps: 1.8000000000000003, min_samples: 3, Clusters: 1, Noise points: 0\n",
      "eps: 1.8000000000000003, min_samples: 4, Clusters: 1, Noise points: 0\n",
      "eps: 1.8000000000000003, min_samples: 5, Clusters: 1, Noise points: 0\n",
      "eps: 1.8000000000000003, min_samples: 6, Clusters: 1, Noise points: 0\n",
      "eps: 1.8000000000000003, min_samples: 7, Clusters: 1, Noise points: 0\n",
      "eps: 1.8000000000000003, min_samples: 8, Clusters: 1, Noise points: 0\n",
      "eps: 1.8000000000000003, min_samples: 9, Clusters: 1, Noise points: 0\n",
      "eps: 1.9000000000000001, min_samples: 2, Clusters: 1, Noise points: 0\n",
      "eps: 1.9000000000000001, min_samples: 3, Clusters: 1, Noise points: 0\n",
      "eps: 1.9000000000000001, min_samples: 4, Clusters: 1, Noise points: 0\n",
      "eps: 1.9000000000000001, min_samples: 5, Clusters: 1, Noise points: 0\n",
      "eps: 1.9000000000000001, min_samples: 6, Clusters: 1, Noise points: 0\n",
      "eps: 1.9000000000000001, min_samples: 7, Clusters: 1, Noise points: 0\n",
      "eps: 1.9000000000000001, min_samples: 8, Clusters: 1, Noise points: 0\n",
      "eps: 1.9000000000000001, min_samples: 9, Clusters: 1, Noise points: 0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "\n",
    "# Define a function to tune DBSCAN and print the number of clusters\n",
    "def tune_dbscan(embeddings, eps_values, min_samples_values):\n",
    "    for eps in eps_values:\n",
    "        for min_samples in min_samples_values:\n",
    "            dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "            labels = dbscan.fit_predict(embeddings)\n",
    "            \n",
    "            # Number of clusters identified (excluding noise points)\n",
    "            n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "            n_noise = list(labels).count(-1)\n",
    "            \n",
    "            print(f\"eps: {eps}, min_samples: {min_samples}, Clusters: {n_clusters}, Noise points: {n_noise}\")\n",
    "\n",
    "# Experiment with different values of eps and min_samples\n",
    "eps_values = np.arange(0.1, 2.0, 0.1)  # Adjust range as needed\n",
    "min_samples_values = range(2, 10)\n",
    "\n",
    "# Fine-tune DBSCAN for OpenAI embeddings\n",
    "print(\"Tuning DBSCAN for OpenAI Model (text-embedding-ada-002):\")\n",
    "# Load the embeddings from the numpy file\n",
    "embeddings_ada = np.load('embeddings_ada.npy')\n",
    "tune_dbscan(embeddings_ada, eps_values, min_samples_values)\n",
    "\n",
    "# Fine-tune DBSCAN for Sentence-Transformer embeddings\n",
    "# print(\"Tuning DBSCAN for Sentence-Transformer Model (all-MiniLM-L6-v2):\")\n",
    "# # Load the embeddings from the numpy file\n",
    "# embeddings_sentence_transformer = np.load('embeddings_sentence_transformer.npy')\n",
    "# tune_dbscan(embeddings_sentence_transformer, eps_values, min_samples_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6222d7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
